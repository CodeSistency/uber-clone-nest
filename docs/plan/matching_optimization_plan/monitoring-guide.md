# ğŸ“Š GuÃ­a de Monitoreo - Sistema de Matching Optimizado

## ğŸ¯ VisiÃ³n General

Esta guÃ­a documenta cÃ³mo monitorear el **Sistema de Matching Optimizado** en producciÃ³n, incluyendo mÃ©tricas clave, alertas y procedimientos de troubleshooting.

---

## ğŸ“ˆ MÃ©tricas Clave

### Latencia y Performance

#### MÃ©tricas Principales
```bash
# Latencia total del matching
GET matching:metrics:total_duration

# Latencia por fase (desarrollo)
console.time('ğŸ” Health Check')    # ~2-5ms
console.time('ğŸ”§ Filters Building') # ~1-3ms
console.time('ğŸ—‚ï¸ Drivers Search')   # ~10-45ms
console.time('ğŸ“ Distance Calculation') # ~5-20ms
console.time('ğŸ§® Scoring')          # ~10-35ms
console.time('ğŸ“Š Response Preparation') # ~2-8ms
```

#### Thresholds Recomendados
| MÃ©trica | Verde (< 50ms) | Amarillo (50-100ms) | Rojo (> 100ms) |
|---------|----------------|-------------------|---------------|
| **Latencia Total** | âœ… Ã“ptimo | âš ï¸ Aceptable | âŒ Problema |
| **Health Check** | < 10ms | 10-20ms | > 20ms |
| **Cache Lookup** | < 5ms | 5-10ms | > 10ms |
| **Database Query** | < 30ms | 30-60ms | > 60ms |

### CachÃ© y Eficiencia

#### MÃ©tricas de CachÃ©
```bash
# Hit rate de cachÃ©
GET cache:hit_rate:drivers_availability
GET cache:hit_rate:drivers_details

# TTL adaptativo en uso
GET cache:ttl_adaptive:active

# TamaÃ±o de cachÃ© comprimida
GET cache:compression:ratio
```

#### Alertas de CachÃ©
```typescript
// Alertas automÃ¡ticas configuradas
cache_hit_rate < 70% â†’ ALERTA: "Cache efficiency baja"
cache_miss_rate > 30% â†’ ALERTA: "Demasiados cache misses"
compression_ratio < 50% â†’ ALERTA: "CompresiÃ³n ineficiente"
```

### Throughput y Escalabilidad

#### MÃ©tricas de Carga
```bash
# Requests por segundo
GET matching:throughput:rps

# Concurrencia mÃ¡xima soportada
GET matching:concurrency:max_supported

# Errores por minuto
GET matching:errors:per_minute
```

#### LÃ­mites de Escalabilidad
- **Concurrencia mÃ¡xima:** 50 requests simultÃ¡neos
- **Throughput objetivo:** 100-200 req/s
- **Error rate mÃ¡ximo:** < 1%

---

## ğŸš¨ Sistema de Alertas

### Alertas CrÃ­ticas (ğŸ”´ AcciÃ³n Inmediata)
```bash
# Sistema caÃ­do
matching:health_check:failed â†’ "ğŸš¨ SISTEMA DE MATCHING CAÃDO"

# Performance crÃ­tica
matching:latency:p95 > 500ms â†’ "ğŸš¨ LATENCIA CRÃTICA DETECTADA"

# Cache completamente fallido
cache:hit_rate < 10% â†’ "ğŸš¨ CACHE COMPLETAMENTE INEFICAZ"
```

### Alertas de Advertencia (ğŸŸ¡ Monitoreo)
```bash
# DegradaciÃ³n de performance
matching:latency:p95 > 200ms â†’ "âš ï¸ LATENCIA ELEVADA"

# Cache con problemas
cache:hit_rate < 60% â†’ "âš ï¸ CACHE BAJO RENDIMIENTO"

# Errores crecientes
matching:errors:rate > 5% â†’ "âš ï¸ TASA DE ERRORES ELEVADA"
```

### Alertas Informativas (ğŸŸ¢ Seguimiento)
```bash
# Cambios en patrones
cache:pattern_change â†’ "â„¹ï¸ PATRÃ“N DE ACCESO CAMBIADO"

# Optimizaciones aplicadas
cache:ttl_adapted â†’ "â„¹ï¸ TTL ADAPTATIVO ACTIVADO"

# Maintenance necesaria
cache:cleanup_needed â†’ "â„¹ï¸ LIMPIEZA DE CACHE RECOMENDADA"
```

---

## ğŸ” Troubleshooting

### Problemas Comunes y Soluciones

#### 1. Latencia Alta
```
SÃ­ntomas: matching:latency:p95 > 200ms

Posibles causas:
ğŸ” Redis lento â†’ Verificar redis-cli ping
ğŸ—„ï¸ DB sobrecargada â†’ Revisar PostgreSQL connections
âš¡ CPU alta â†’ Verificar load average
ğŸ“¡ Red lenta â†’ Verificar network latency

SoluciÃ³n:
1. Verificar mÃ©tricas: npm run test:matching
2. Revisar logs: NODE_ENV=development
3. Scale infrastructure si necesario
```

#### 2. Cache Hit Rate Baja
```
SÃ­ntomas: cache:hit_rate < 60%

Posibles causas:
ğŸ”„ Datos expirando muy rÃ¡pido â†’ Aumentar TTL base
ğŸ“Š Patrones de acceso cambiantes â†’ Reset adaptive TTL
ğŸ’¾ Memoria insuficiente â†’ Aumentar Redis memory
ğŸ¯ Cache key collisions â†’ Revisar key strategy

SoluciÃ³n:
1. Monitor access patterns
2. Adjust TTL settings
3. Increase Redis memory
4. Optimize key naming
```

#### 3. Errores de Consistencia
```
SÃ­ntomas: matching:consistency < 90%

Posibles causas:
ğŸ”„ Race conditions â†’ Revisar paralelizaciÃ³n
ğŸ’¾ Cache stale â†’ Invalidar manualmente
ğŸ—„ï¸ DB inconsistencies â†’ Verificar constraints
âš™ï¸ ConfiguraciÃ³n errÃ³nea â†’ Validar environment

SoluciÃ³n:
1. Run consistency tests
2. Clear cache manually
3. Check database integrity
4. Validate configuration
```

#### 4. Alta Concurrencia ProblemÃ¡tica
```
SÃ­ntomas: matching:concurrency:errors > 10%

Posibles causas:
ğŸ”¢ LÃ­mite de concurrencia excedido â†’ Aumentar lÃ­mite
âš¡ CPU throttling â†’ Verificar resources
ğŸ’¾ Memory pressure â†’ Monitor RAM usage
ğŸ”’ Locks excesivos â†’ Optimizar locking

SoluciÃ³n:
1. Increase concurrency limit gradually
2. Monitor system resources
3. Implement circuit breaker
4. Optimize locking strategy
```

---

## ğŸ“Š Dashboards de Monitoreo

### Dashboard Principal
```bash
# MÃ©tricas en tiempo real
watch -n 5 'redis-cli --raw keys "matching:metrics:*" | head -10'

# Health check continuo
while true; do curl -s http://localhost:3000/health && echo " âœ…" || echo " âŒ"; sleep 30; done
```

### Grafana Dashboard Recomendado
```json
{
  "title": "Matching System Performance",
  "panels": [
    {
      "title": "Latency P95",
      "targets": ["matching:metrics:latency:p95"]
    },
    {
      "title": "Cache Hit Rate",
      "targets": ["cache:hit_rate:drivers_*"]
    },
    {
      "title": "Throughput",
      "targets": ["matching:throughput:rps"]
    },
    {
      "title": "Error Rate",
      "targets": ["matching:errors:per_minute"]
    }
  ]
}
```

---

## ğŸ› ï¸ Herramientas de DiagnÃ³stico

### Comandos Ãštiles
```bash
# Ver mÃ©tricas actuales
redis-cli keys "matching:metrics:*" | xargs redis-cli mget

# Limpiar cachÃ© para testing
redis-cli flushdb  # âš ï¸ Solo en desarrollo

# Ver logs de aplicaciÃ³n
tail -f logs/combined.log | grep "MATCHING\|CACHE"

# Test de carga bÃ¡sico
ab -n 1000 -c 10 http://localhost:3000/rides/match
```

### Scripts de Monitoreo
```bash
#!/bin/bash
# monitoring-script.sh

echo "ğŸ” Sistema de Matching - Status Report"
echo "====================================="

# Health check
echo "ğŸ¥ Health Check:"
curl -s http://localhost:3000/health | jq .status || echo "âŒ Failed"

# Performance metrics
echo -e "\nğŸ“Š Performance Metrics:"
redis-cli get matching:metrics:latency:p95 2>/dev/null || echo "N/A"

# Cache status
echo -e "\nğŸ’¾ Cache Status:"
redis-cli --raw keys "cache:*" | wc -l 2>/dev/null || echo "Redis unavailable"

echo -e "\nâœ… Report completed"
```

---

## ğŸ“ˆ MÃ©tricas de Ã‰xito

### KPIs Objetivo
- **Disponibilidad:** 99.9% uptime
- **Latencia P95:** < 200ms
- **Cache Hit Rate:** > 75%
- **Error Rate:** < 1%
- **Consistencia:** > 95%

### Seguimiento Continuo
```bash
# Reporte semanal automÃ¡tico
0 9 * * 1 /path/to/monitoring-script.sh | mail -s "Weekly Matching Report" admin@company.com

# Alertas en tiempo real (ejemplo con Prometheus)
groups:
  - name: matching_alerts
    rules:
      - alert: HighLatency
        expr: matching_latency_p95 > 200
        for: 5m
        labels:
          severity: warning
```

---

## ğŸš€ Optimizaciones Proactivas

### Maintenance Programado
```bash
# Limpieza semanal de mÃ©tricas antiguas
redis-cli keys "matching:metrics:*" | xargs redis-cli del

# OptimizaciÃ³n mensual de Ã­ndices DB
# (Ejecutar queries de optimization de PostgreSQL)

# Reset de patrones de cachÃ© trimestral
redis-cli del cache:patterns:*
```

### Auto-scaling Basado en MÃ©tricas
```yaml
# Kubernetes HPA basado en mÃ©tricas personalizadas
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: matching-service-hpa
spec:
  metrics:
  - type: External
    external:
      metric:
        name: matching_latency_p95
        selector:
          matchLabels:
            service: matching
      target:
        type: Value
        value: "200"
```

---

## ğŸ“ Contactos y EscalaciÃ³n

### Equipos Responsables
- **Desarrollo:** dev-team@company.com
- **Infraestructura:** infra@company.com
- **Producto:** product@company.com

### Procedimientos de EscalaciÃ³n
1. **P1 (CrÃ­tico):** < 15 minutos - Sistema caÃ­do
2. **P2 (Alto):** < 1 hora - Performance crÃ­tica
3. **P3 (Medio):** < 4 horas - DegradaciÃ³n significativa
4. **P4 (Bajo):** < 24 horas - Problemas menores

---

*Esta guÃ­a se actualiza automÃ¡ticamente con cada deployment. Ãšltima actualizaciÃ³n: 28 de septiembre de 2025*
